# Adversarial Machine Learning

An Adversarial Attack is a technique to find a perturbation that changes the prediction of a machine learning model. The perturbation can be very small and imperceptible to human eyes.

## ğŸ“„ Papers

- [Papers with Code - Adversarial Attack](https://paperswithcode.com/task/adversarial-attack)

## ğŸ“° Articles

- [Adversarial machine learning](https://en.wikipedia.org/wiki/Adversarial_machine_learning) on Wikipedia

## ğŸ› ï¸ Libraries, frameworks, etc.

| Title | Description, Information |
| :---:         |          :--- |
|[TextAttack ğŸ™](https://github.com/QData/TextAttack)|TextAttack is a Python framework for adversarial attacks, data augmentation, and model training in NLP.|
|[TextFlint](https://github.com/textflint/textflint#usage)|<p>**Unified Multilingual Robustness Evaluation Toolkit for Natural Language Processing**</p><p>TextFlint is a multilingual robustness evaluation platform for natural language processing, which unifies text transformation, sub-population, adversarial attack,and their combinations to provide a comprehensive robustness analysis. So far, TextFlint supports 13 NLP tasks.</p><ul><li> ğŸ“„ **Paper:** [TextFlint: Unified Multilingual Robustness Evaluation Toolkit for Natural Language Processing](https://aclanthology.org/2021.acl-demo.41.pdf)</li><li>[TextFlint Documentation](https://textflint.readthedocs.io/en/latest/)</li></ul>|
